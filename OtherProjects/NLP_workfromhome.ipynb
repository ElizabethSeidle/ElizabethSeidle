{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP: LDA and Sentiment analyses of text survey data\n",
    "\n",
    "Helpful links: \n",
    "- https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n",
    "- https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "survey_data_raw = pd.read_excel(\"Working Remotely Survey.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey Question: Do you have any additional concerns about\\nworking remotely that you would like to share at this time?\n",
    "\n",
    "##### 1. Prep data from raw data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_concerns = survey_data_raw.iloc[:,51:52]\n",
    "add_concerns.rename(columns={ add_concerns.columns[0]: \"Additional_Concerns\" }, inplace = True)\n",
    "add_concerns = add_concerns.dropna()\n",
    "add_concerns = add_concerns.reset_index(drop=True)\n",
    "add_concerns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.  Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "#stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    #sentence=sentence.replace('{html}',\"\") \n",
    "    sentence = str(TextBlob(sentence).correct()) #correct spelling\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)\n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "add_concerns['cleanText']=add_concerns['Additional_Concerns'].map(lambda s:preprocess(s)) \n",
    "\n",
    "#remove common words we don't want included in LDA topics, as needed (context spefic)\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('working', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('work', '') \n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('home', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('would', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('remotely', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('remote', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('concerns', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('think', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('able', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('concern', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('office', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('day', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('feel', '')\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].str.replace('also', '')\n",
    "\n",
    "add_concerns['cleanText'] = add_concerns['cleanText'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Topic Modelling\n",
    "\n",
    "https://honingds.com/blog/topic-modeling-latent-dirichlet-allocation-lda/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install pyLDAvis\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "#Create a Gensim dictionary from the tokenized data \n",
    "cleaned = add_concerns['cleanText']\n",
    "#Creating term dictionary of corpus, where each unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(cleaned)\n",
    "#Filter terms which occurs in less than 1 answer and more than 80% of the answers.\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "#convert the dictionary to a bag of words corpus \n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in cleaned]\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 8, id2word=dictionary, passes=30)\n",
    "ldamodel.save('model_combined.gensim')\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "   print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic-Words matrix contains the probability distribution of words generated from those topics. By running the LDA algorithm on the above data produces the above outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_document_topics = ldamodel.get_document_topics(corpus[0])\n",
    "print(get_document_topics) #entry 0 is x% related to topic n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominant_topic(ldamodel, corpus, texts):\n",
    "     #Function to find the dominant topic in each review\n",
    "     sent_topics_df = pd.DataFrame() \n",
    "     # Get main topic in each review\n",
    "     for i, row in enumerate(ldamodel[corpus]):\n",
    "         row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "         # Get the Dominant topic, Perc Contribution and Keywords for each review\n",
    "         for j, (topic_num, prop_topic) in enumerate(row):\n",
    "             if j == 0:  # =&gt; dominant topic\n",
    "                 wp = ldamodel.show_topic(topic_num,topn=4)\n",
    "                 topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                 sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "             else:\n",
    "                 break\n",
    "     sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "     contents = pd.Series(texts)\n",
    "     sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "     return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic = dominant_topic(ldamodel=ldamodel, corpus=corpus, texts=add_concerns['Additional_Concerns']) \n",
    "df_dominant_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_concerns_2 = survey_data_raw.iloc[:,51:52]\n",
    "add_concerns_2.rename(columns={ add_concerns_2.columns[0]: \"Additional_Concerns\" }, inplace = True)\n",
    "add_concerns_2 = add_concerns_2.dropna()\n",
    "add_concerns_2 = add_concerns_2.reset_index(drop=True)\n",
    "add_concerns_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ReviewText):\n",
    "    ReviewText = ReviewText.str.replace(\"(<br/>)\", \"\")\n",
    "    ReviewText = ReviewText.str.replace('(<a).*(>).*(</a>)', '')\n",
    "    ReviewText = ReviewText.str.replace('(&amp)', '')\n",
    "    ReviewText = ReviewText.str.replace('(&gt)', '')\n",
    "    ReviewText = ReviewText.str.replace('(&lt)', '')\n",
    "    ReviewText = ReviewText.str.replace('(\\xa0)', ' ')  \n",
    "    return ReviewText\n",
    "\n",
    "add_concerns_2['Additional_Concerns'] = preprocess(add_concerns_2['Additional_Concerns'])\n",
    "\n",
    "add_concerns_2['polarity'] = add_concerns_2['Additional_Concerns'].map(lambda text: TextBlob(text).sentiment.polarity) #calculate sentiment polarity which lies in the range of [-1,1] where 1 means positive sentiment and -1 means a negative sentiment.\n",
    "add_concerns_2['review_len'] = add_concerns_2['Additional_Concerns'].astype(str).apply(len) #Create new feature for the length of the review\n",
    "add_concerns_2['word_count'] = add_concerns_2['Additional_Concerns'].apply(lambda x: len(str(x).split())) #Create new feature for the word count of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('3 random reviews with the relatively high positive sentiment polarity: \\n')\n",
    "cl = add_concerns_2.loc[add_concerns_2.polarity >= 0.6, ['Additional_Concerns']].sample(3).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('3 random reviews with the most neutral sentiment(zero) polarity: \\n')\n",
    "cl = add_concerns_2.loc[add_concerns_2.polarity == 0.0, ['Additional_Concerns']].sample(3).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('3 reviews with the most negative polarity: \\n')\n",
    "cl = add_concerns_2.loc[add_concerns_2.polarity <= 0.8, ['Additional_Concerns']].sample(3).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly==4.9.0\n",
    "#!pip install cufflinks\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "add_concerns_2['polarity'].iplot(\n",
    "    kind='hist',\n",
    "    bins=50,\n",
    "    xTitle='polarity',\n",
    "    linecolor='black',\n",
    "    yTitle='count',\n",
    "    title='Sentiment Polarity Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 Words after removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "common_words = get_top_n_words(add_concerns_2['Additional_Concerns'], 20)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "    \n",
    "df2 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df2.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in review after removing stop words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top BiGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "common_words = get_top_n_bigram(add_concerns_2['Additional_Concerns'], 20)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "    \n",
    "df4 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df4.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in review after removing stop words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "common_words = get_top_n_trigram(add_concerns_2['Additional_Concerns'], 20)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "\n",
    "df6 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df6.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in review after removing stop words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional storage code - as reference, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "add_concerns['Additional_Concerns'] = add_concerns['Additional_Concerns'].apply(word_tokenize)\n",
    "add_concerns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "add_concerns['stemmed'] = add_concerns['Additional_Concerns'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "add_concerns['processed'] = add_concerns['stemmed'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "add_concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word cloud of product categories\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "text = \" \".join(i for i in add_concerns.processed)\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=70, max_words=15, background_color=\"black\").generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
